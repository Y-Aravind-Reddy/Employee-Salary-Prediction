# -*- coding: utf-8 -*-
"""Salary Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o-NymBfsL_CA4b1AxP5YqP2MqrV34fh3

#Employee Salary prediction assignment.
Objective: Understand salary determinants based on experience and qualifications. The objective of this analysis is to explore the Salary Data.csv dataset to understand the factors influencing salaries and build a predictive model. The goal is to identify key drivers of salary, such as education level, job title, years of experience, and other demographic features, and to develop a regression model that can predict salaries based on these inputs
"""

# Import necessary libraries
import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Ignore all warnings
warnings.filterwarnings("ignore")
# Ignore specific warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", message="This is a specific warning to ignore")

df=pd.read_csv("/content/archive (2).zip")

# Display basic information about the dataset
print("Dataset Overview:")
print(df.head())
print("--------------------------------------------------")
print("\nDataset shape:")
print(df.shape)
print("\nDataset Info:")
print(df.info())
print("--------------------------------------------------")
print("\nSummary Statistics:")
print(df.describe())
print("--------------------------------------------------")
# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Define numerical features
numerical_features = ['Age', 'Years of Experience', 'Salary']

# Print missing values BEFORE applying the median imputation
print("Missing Values BEFORE Imputation:")
for col in numerical_features:
    missing_count = df[col].isnull().sum()
    print(f"{col}: {missing_count} missing values")
print("\n")
# Handle missing values by filling with the median
for col in numerical_features:
    if df[col].isnull().sum() > 0:
        print(f"Filling {col} with median: {df[col].median()}")
        df[col].fillna(df[col].median(), inplace=True)

# Print missing values AFTER applying the median imputation
print("\nMissing Values AFTER Imputation:")
for col in numerical_features:
    missing_count = df[col].isnull().sum()
    print(f"{col}: {missing_count} missing values")

categorical_features = ['Gender', 'Education Level', 'Job Title']

# Print missing values BEFORE applying the mode imputation
print("Missing Values BEFORE Imputation:")
for col in categorical_features:
    missing_count = df[col].isnull().sum()
    print(f"{col}: {missing_count} missing values")
print("\n")
# Handle missing values by filling with the mode
for col in categorical_features:
    if df[col].isnull().sum() > 0:
        print(f"Filling {col} with mode: {df[col].mode()[0]}")
        df[col].fillna(df[col].mode()[0], inplace=True)

# Print missing values AFTER applying the mode imputation
print("\nMissing Values AFTER Imputation:")
for col in categorical_features:
    missing_count = df[col].isnull().sum()
    print(f"{col}: {missing_count} missing values")

print("\nVisualizing Numerical Feature Distributions:")
df[numerical_features].hist(bins=20, figsize=(15, 8))
plt.suptitle("Histograms of Numerical Features")
plt.show()

plt.figure(figsize=(15, 8))
for i, col in enumerate(numerical_features, 1):
    plt.subplot(1, 3, i)
    sns.boxplot(data=df[col])
    plt.title(f"Box Plot of {col}")
plt.tight_layout()
plt.show()

correlation_matrix = df[numerical_features].corr()
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Correlation Matrix")
plt.show()

print("\nAnalyzing Categorical Features:")
for column in categorical_features:
    print(f"\nFrequency Table for {column}:")
    print(df[column].value_counts())

    # Bar plot for categorical variables with red color
    plt.figure(figsize=(8, 4))
    sns.countplot(data=df, x=column, color='blue')
    plt.title(f"Distribution of {column}")
    plt.xticks(rotation=45)
    plt.show()

# Encode categorical variables using one-hot encoding
# Remove 'Salary' from numerical_features
numerical_features = ['Age', 'Years of Experience']
# Handle unknown categories in the OneHotEncoder
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)
    ]
)

target = 'Salary'
features = df.drop(columns=[target])

# Split the data into training and testing sets
X = features
y = df[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Build and Train the Linear Regression Model

# Create a pipeline with preprocessing and regression
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', LinearRegression())
])

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_pred

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\nModel Evaluation:")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"R-squared (R2): {r2:.2f}")

coefficients = pd.DataFrame({
    'Feature': model.named_steps['preprocessor'].get_feature_names_out(),  # Get feature names from preprocessor
    'Coefficient': model.named_steps['regressor'].coef_  # Access coef_ from the 'regressor' step
}).sort_values(by='Coefficient', ascending=False)

print(coefficients)

# Visualize actual vs predicted values
plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_test, y=y_pred)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.xlabel("Actual Salary")
plt.ylabel("Predicted Salary")
plt.title("Actual vs Predicted Salary")
plt.show()

"""Expected Output EDA Insights : Salary is positively correlated with Years of Experience and Age. Higher education levels (e.g., Master's, PhD) tend to correlate with higher salaries. Certain job titles (e.g., Director, Senior roles) are associated with significantly higher salaries. Preprocessed Dataset : Categorical variables are one-hot encoded. Numerical features are standardized. Model Evaluation : The Mean Squared Error (MSE) and R-squared (RÂ²) provide insights into the model's predictive performance. A scatter plot visualizes how well the model predicts salary."""